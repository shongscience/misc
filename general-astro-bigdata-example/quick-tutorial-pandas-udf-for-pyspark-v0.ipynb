{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Tutorial of PandasUDF for PySpark 3.x\n",
    "\n",
    "Thanks to the below source author \n",
    "\n",
    "Source: https://towardsdatascience.com/distributed-processing-with-pyarrow-powered-new-pandas-udfs-in-pyspark-3-0-8f1fe4c15208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list |grep pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import pandas as pd\n",
    "\n",
    "from astropy.utils.exceptions import AstropyWarning\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_rows', 300)\n",
    "    \n",
    "np.seterr(all='ignore')\n",
    "warnings.simplefilter('ignore', category=AstropyWarning)\n",
    "\n",
    "# https://github.com/gbrammer/eazy-py\n",
    "import eazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import h5py\n",
    "#from netCDF4 import Dataset\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from functools import reduce\n",
    "import operator\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot settings\n",
    "#plt.rc('font', family='serif') \n",
    "#plt.rc('font', serif='Times New Roman') \n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n",
      "eazy                      0.6.8               \n"
     ]
    }
   ],
   "source": [
    "!pip list |grep eazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 ms, sys: 3.6 ms, total: 18.2 ms\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PySpark packages\n",
    "from pyspark import SparkContext   \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import Row\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"spark-shell\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"50\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setCheckpointDir(\"hdfs://spark00:54310/tmp/checkpoints\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 500)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Tutorial of PandasUDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = spark.createDataFrame([(1, 5), (2, 7), (2, 8), (2, 10), (3, 18), (3, 22), (4, 36)], \\\n",
    "                                  [\"index\", \"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|index|weight|\n",
      "+-----+------+\n",
      "|    1|     5|\n",
      "|    2|     7|\n",
      "|    2|     8|\n",
      "|    2|    10|\n",
      "|    3|    18|\n",
      "|    3|    22|\n",
      "|    4|    36|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function definition and the UDF creation\n",
    "@pandas_udf(\"int\")\n",
    "def weight_avg_udf(weight: pd.Series) -> float:\n",
    "    return weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|weight_avg_udf(weight)|\n",
      "+----------------------+\n",
      "|                    15|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.select(weight_avg_udf(dataframe['weight'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------+\n",
      "|index|weight_avg_udf(weight)|\n",
      "+-----+----------------------+\n",
      "|    1|                     5|\n",
      "|    2|                     8|\n",
      "|    3|                    20|\n",
      "|    4|                    36|\n",
      "+-----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregation Process on Pandas UDF\n",
    "dataframe.groupby(\"index\").agg(weight_avg_udf(dataframe['weight'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the windowed results\n",
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy('index') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|index|weight|avg_weight|\n",
      "+-----+------+----------+\n",
      "|    1|     5|         5|\n",
      "|    2|     7|         8|\n",
      "|    2|    10|         8|\n",
      "|    2|     8|         8|\n",
      "|    3|    18|        20|\n",
      "|    3|    22|        20|\n",
      "|    4|    36|        36|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.withColumn('avg_weight', weight_avg_udf(dataframe['weight']).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped_map using `applyInPandas` which is quite different from `apply` using pyspark's native UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|index|weight|\n",
      "+-----+------+\n",
      "|    1|     0|\n",
      "|    2|    -1|\n",
      "|    2|     1|\n",
      "|    2|     0|\n",
      "|    3|     2|\n",
      "|    3|    -2|\n",
      "|    4|     0|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def weight_map_udf(pandas_dataframe):\n",
    "    weight = pandas_dataframe.weight\n",
    "    return pandas_dataframe.assign(weight=weight - weight.mean())\n",
    "dataframe.groupby(\"index\").applyInPandas(weight_map_udf, schema=\"index int, weight int\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Learn more about UDF and PandasUDF!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
